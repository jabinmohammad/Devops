DOCKER SWARM:
CLUSTER 
NODES
CONTAINER
APP

C: CLUSTER
N: NODE
P: POD
C: CONTAINER
A: APPLICATION

NOTE: k8s dont communicate with containers.
it communicate with pods.

COMPONENTS:
MASTER NODE:
1. API SERVER: its for communicating with cluster, it takes command executes and gives output.
2. ETCD: its a DB of cluster, all the cluster info will store here.
3. SCHEDULER: schedules pods on worker nodes, based on hardware resources.
4. CONTROLLER: used to control the k8s objects.
1. cloud controllers
2. kube controllers

WORKER NODE:
KUBELET: its an agent used to communicate with master.
KUBEPROXY: it deals with nlw.
POD: its a group of containers.


There are multiple ways to setup kubernetes cluster.

1.SELF MANAGER K8'S CLUSTER
a.mini kube (single node cluster)
b.kubeadm(multi node cluster)
c. KOPS

2. CLOUD MANAGED K8'S CLUSTER
a. AWS EKS
b.AZURE AKS
c.GCP GKS
d.IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.



PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE: 
kubectl run pod1 --image rahamshaik/paytmtrain:latest
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

Declarative:
vim abc.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f abc.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DRAWBACK:
if we delete the pod we cant retrive.
all the load will be handled by single pod.


REPLICA SET:
it will create same pod of multiple replicas.
if we delete one pod it will create automatically.
we can distribute the load also.


LABLE: assing to a pod for identification. to work with them as single unit.
SELECTOR: used to identify the pod with same label.

kubectl api-resources

REPLICASET:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl create -f abc.yml

kubectl get rs
kubectl get rs -o wide
kubectl describe rs swiggy-rs
kubectl delete rs swiggy-rs
kubectl edit rs/swiggy-rs



SCALING: 

SCALE-IN: Increasing the count of pods
kubectl scale rs/swiggy-rs --replicas=10

SCALE-OUT: Decreasing the count of pods
kubectl scale rs/swiggy-rs --replicas=5

SCALING FOLLOWS LIFO PATTERN:
LIFO: LAST IN FIRST OUT
the pod which is created will be deleted first automatically when we scale out.


DEPLOYMENT:
it will do all operations link RS.
it will do roll back which cannot be done in rs.

rs -- > pods
deployment -- > rs -- > pods

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl get deploy
kubectl get deploy -o wide
kubectl describe deploy swiggy-rs
kubectl edit deploy/swiggy-rs
kubectl delete deploy swiggy-r
kubectl delete po --all


KUBECOLOR:

wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/
kubecolor get po

HISTORY:
1  vim minikube.sh
    2  sh minikube.sh
    3  vim abc.yml
    4  kubectl create -f abc.yml
    5  vim abc.yml
    6  kubectl create -f abc.yml
    7  kubectl get po
    8  kubectl delete pod raham
    9  kubectl get po
   10  vim abc.yml
   11  kubectl get po
   12  kubectl create -f abc.yml
   13  cat abc.yml
   14  vim abc.yml
   15  kubectl create -f abc.yml
   16  kubectl get rs
   17  kubectl ap-resources
   18  kubectl api-resources
   19  kubectl get rs -o wide
   20  kubectl describe rs train-rs
   21  kubectl get po
   22  kubectl get rs -o wide
   23  kubectl delete pod train-rs-gshzk
   24  kubectl get po
   25  kubectl delete pod train-rs-767lv
   26  kubectl get po
   27  kubectl scale rs/train-rs --replicas=10
   28  kubectl get po
   29  kubectl scale rs/train-rs --replicas=5
   30  kubectl get po
   31  kubectl describe rs tarin-rs
   32  kubectl describe rs train-rs
   33  kubectl edit rs/train-rs
   34  kubectl describe rs train-rs
   35  kubectl get po
   36  kubectl describe pod train-rs-24xvk | grep -i image
   37  kubectl describe pod train-rs-24xvk
   38  kubectl describe rs train-rs
   39  kubectl describe pod
   40  kubectl get po
   41  kubectl describe pod train-rs-24xvk
   42  kubectl describe pod train-rs-gq9bb
   43  kubectl describe pod train-rs-m6gv5
   44  kubectl get po
   45  kubectl run pod1 --image nginx
   46  kubectl run pod2 --image nginx
   47  kubectl run pod3 --image nginx
   48  kubectl get po
   49  kubectl delete pod -l app=train
   50  kubectl delete rs train-rs
   51  kubectl get po
   52  kubectl get po -o wide
   53  kubectl delete po --all
   54  vim abc.yml
   55  kubectl create -f abc.yml
   56  kubectl get deploy
   57  kubectl get rs
   58  kubectl get po
   59  kubectl describe deploy train-rs
   60  kubectl edit deploy/train-rs
   61  kubectl get po
   62  kubectl describe deploy train-rs
   63  kubectl describe po
   64  kubectl edit deploy/train-rs
   65  kubectl get po
   66  kubectl describe po
   67  kubectl describe po | grep -i Image
   68  kubectl scale deploy/train-rs --replicas=10
   69  kubectl get po
   70  wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
   71  tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
   72  ./kubecolor
   73  chmod +x kubecolor
   74  mv kubecolor /usr/local/bin/
   75  kubecolor get po
   76  kubectl get rs
   77  kubecolor get rs
   78  kubecolor get po
   79  history
   80  kubecolor describe deploy train-rs
   81  kubecolor get po
   82  kubecolor logs train-rs-6cddf5c876-6ws5z
   83  kubecolor logs ttrain-rs-6cddf5c876-wh6mr
   84  kubecolor logs train-rs-6cddf5c876-wh6mr
   85  kubecolor logs train-rs-6cddf5c876-wh6mr -c cont1
   86  history
root@ip-172-31-10-159:~#

There are multiple ways to setup kubernetes cluster.

1.SELF MANAGER K8'S CLUSTER
a.mini kube (single node cluster)
b.kubeadm(multi node cluster)
c. KOPS

2. CLOUD MANAGED K8'S CLUSTER
a. AWS EKS
b.AZURE AKS
c.GCP GKS
d.IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE: 
kubectl run pod1 --image rahamshaik/paytmtrain:latest
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

Declarative:
vim abc.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f abc.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

HISTORY:
  1  apt update -y
    2  apt upgrade -y
    3  sudo apt install curl wget apt-transport-https -y
    4  sudo curl -fsSL https://get.docker.com -o get-docker.sh
    5  ll
    6  sh get-docker.sh
    7  sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
    8  ll
    9  sudo mv minikube-linux-amd64 /usr/local/bin/minikube
   10  chmod +x /usr/local/bin/minikube
   11  minikube version
   12  sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux                                                                                                                                                           /amd64/kubectl"
   13  ll
   14  sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/k                                                                                                                                                           ubectl.sha256"
   15  echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
   16  sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
   17  kubectl version
   18  kubectl version --client
   19  minikube start --driver=docker --force
   20  minikube status
   21  kubectl get pod
   22  kubectl get pods
   23  kubectl get po
   24  kubectl run pod1 --image rahamshaik/paytmtrain:latest
   25  kubectl get pod
   26  kubectl get pods
   27  kubectl get po
   28  kubectl describe pod pod1
   29  kubectl get po
   30  kubectl delete pod pod1
   31  kubectl get po
   32  kubectl run pod1 --image ubuntu
   33  kubectl get po
   34  kubectl delete pod pod1
   35  kubectl run raham --image nginx
   36  kubectl get po
   37  kubectl describe pod raham
   38  kubectl get po -o wide
   39  kubectl delete pod raham
   40  vim abc.yml
   41  kubectl create -f abc.yml
   42  kubectl get po
   43  kubectl get po -o wide
   44  kubectl describe pod pod1
   45  kubectl delete pod pod1
   46  history
===========================================================
KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, -------------


Minikube -- > single node cluster
All the pods on single node 
kOps, also known as Kubernetes operations.
it is an open-source tool that helps you create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
kOps is mostly used in deploying AWS and GCE Kubernetes clusters. 
But officially, the tool only supports AWS. Support for other cloud providers (such as DigitalOcean, GCP, and OpenStack) are in the beta stage.


ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.


STEP-1: GIVING PERMISSIONS
IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATEING BUCKET 
aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahams.k8s.local
 * edit your node instance group: kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

kubectl describe node node_id
kops delete cluster --name rahams.k8s.local --yes

HISTORY:
1  aws configure
    2  curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd6                                                           4/kubectl"
    3  wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
    4  chmod +x kops-linux-amd64 kubectl
    5  mv kubectl /usr/local/bin/kubectl
    6  mv kops-linux-amd64 /usr/local/bin/kops
    7  kops version
    8  kubectl version
    9  vim .bashrc
   10  source .bashrc
   11  kops version
   12  kubectl version
   13  aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
   14  aws s3api create-bucket --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1
   15  aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --vers                                                           ioning-configuration Status=Enabled
   16  export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local
   17  aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
   18  aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --vers                                                           ioning-configuration Status=Enabled
   19  export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local
   20  kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medi                                                           um --node-count=2 --node-size t2.micro
   21  kops update cluster --name rahams.k8s.local --yes --admin
   22  kops validate cluster --wait 10m
   23  kops get cluster
   24  kubectl get no
   25  kops edit ig --name=rahams.k8s.local nodes-us-east-1a
   26  kops update cluster --name rahams.k8s.local --yes --admin
   27  kops rolling-update cluster --yes
   28  kops edit ig --name=rahams.k8s.local master-us-east-1a
   29  kops update cluster --name rahams.k8s.local --yes --admin
   30  kops rolling-update cluster --yes
   31  kops edit ig --name=rahams.k8s.local nodes-us-east-1a
   32  kubectl get no
   33  vim abc.yml
   34  kubectl get po
   35  kubectl create -f abc.yml
   36  kubectl get po -o wide
   37  kubectl scale deploy/swiggyrs --replicas=8
   38  kubectl scale deploy/swiggy-rs --replicas=8
   39  kubectl get po -o wide
   40  kubectl describe node i-0c3a6f2269bc5af42
   41  kops get cluster
   42  kops delete cluster --name rahams.k8s.local --yes
   43  history

==================================================================================================================

NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER: HOUSE
NAMESPACES: ROOM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one namespace to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods


NOTE: By deleting  the ns all objects also gets deleted.
in real time we use rbac concept to restrict the access from one namespace to another.
so users cant access/delete ns, because of the restriction we provide.
we create roles and rolebind for the users.


SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: sv1
spec:
  type: ClusterIP
  selector:
    app: swiggy
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: NodePort
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC & SSH)
DRAWBACK:
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80


Multiple Ports:
Kubernetes allows you to define multiple ports for both deployments and services. This is useful for applications that use different ports for various functionalities.
(You can specify multiple ports in both sections of your YAML file, separated by commas.)


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
        - containerPort: 8080  # Web traffic
        - containerPort: 5000  # API server

---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
  - port: 80  # External for web traffic
    targetPort: 8080
  - port: 8081  # External for API (optional)
    targetPort: 5000




HISTORY:

 1  vim kops.sh
    2  vim .bashrc
    3  source .bashrc
    4  sh kops.sh
    5  vim kops.sh
    6  sh kops.sh
    7  kops validate cluster --wait 10m
    8  cat kops.sh
    9  export KOPS_STATE_STORE=s3://devopsbynraeshit887766.k8s.local
   10  kops validate cluster --wait 10m
   11  kubectl get ns
   12  kubectl run pod1 --image nginx
   13  kubectl describe pod pod1
   14  kubectl get po
   15  kubectl get ns
   16  kubectl get po -n kube-node-lease
   17  kubectl get po -n kube-public
   18  kubectl get po -n kube-system
   19  kubectl get po --all
   20  kubectl get po -A
   21  kubectl create ns dev
   22  kubectl get ns
   23  kubectl config set-context --current --namspace=dev
   24  kubectl config set-context --current --namespace=dev
   25  kubectl run devpod1 --image nginx
   26  kubectl run devpod2 --image nginx
   27  kubectl run devpod3 --image nginx
   28  kubectl get po
   29  kubectl view config
   30  kubectl config view
   31  kubectl config view --minfy
   32  kubectl config view --minify
   33  kubectl get po
   34  kubectl get po -A
   35  kubectl create ns test
   36  kubectl config set-context --curent --namespace=test
   37  kubectl config set-context --current --namespace=test
   38  kubectl run testpod1 --image nginx
   39  kubectl run testpod2 --image nginx
   40  kubectl run testpod3 --image nginx
   41  kubectl get po
   42  kubectl get po -n dev
   43  kubectl delete po devpod1 -n dev
   44  kubectl get po -n dev
   45  kubectl delete ns dev
   46  kubectl delete ns test
   47  vim abc.yml
   48  kubectl api-resources
   49  vim abc.yml
   50  kubectl get svc
   51  kubectl get svc -A
   52  kubectl create -f abc.yml
   53  kubectl config set-context --current --namespace=default
   54  kubectl create -f abc.yml
   55  kubectl get deploy,svc
   56  kubectl delete -f abc.yml
   57  vim abc.yml
   58  kubectl create -f abc.yml
   59  vim abc.yml
   60  kubectl create -f abc.yml
   61  kubectl get deploy
   62  kubectl delete deploy swiggy-deploy
   63  kubectl create -f abc.yml
   64  kubectl delete -f abc.yml
   65  kubectl create -f abc.yml
   66  kubectl get deploy,svc
   67  vim abc.yml
   68  kubectl apply -f abc.yml
   69  kubectl delete -f abc.yml
   70  vim abc.yml
   71  kubectl create -f abc.yml
   72  kubectl get deploy,svc
   73  kops get cluster
   74  kops delete cluster --name rahams.k8s.local --yes
   75  history
[root@ip-172-31-45-202 ~]#

